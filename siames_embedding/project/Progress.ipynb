{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce4a729-8379-44dd-ac86-5833fcebc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Embedding Network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b51e15d6-881e-464c-b82c-0d2bcf9ea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, datamap, y_col = \"y\"):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.datamap = datamap\n",
    "        self.y_col = y_col\n",
    "\n",
    "        # 임베딩 레이어 초기화\n",
    "        for k, v in datamap.items() :\n",
    "            if v == \"linear\" :\n",
    "                setattr(self, k, nn.Linear(num_linear_features, embedding_dim))\n",
    "            elif v == \"onehot\" :\n",
    "                setattr(self, k, nn.Embedding(num_sparse_features, embedding_dim))\n",
    "            elif v == \"multihot\" :\n",
    "                setattr(self, k, nn.Embedding(num_sparse_features, embedding_dim))\n",
    "        fc1_embedding_dim = len(datamap) - 1\n",
    "        # 다층 퍼셉트론(MLP) 레이어 초기화 \n",
    "        self.fc1 = nn.Linear(embedding_dim * fc1_embedding_dim, hidden_dim)  # 임베딩된 특성이 3개이므로 *3\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def _get_multihot_embedding(self, embedding, method_multihot) :\n",
    "\n",
    "        if method_multihot == \"mean\" :\n",
    "            # 멀티-핫 특성의 임베딩 평균\n",
    "            multihot_embedding = lambda x : torch.mean(embedding(x), dim = 1)\n",
    "        elif method_multihot == \"sum\" :\n",
    "            # 각 인덱스에 대한 임베딩 벡터를 더하여 합산\n",
    "            multihot_embedding = lambda x : torch.sum(embedding(x), dim = 1)\n",
    "            # multi_hot_embedded = torch.sum(embedding(x[k]), dim=1)\n",
    "        elif method_multihot == \"weighted_mean\" :\n",
    "            # 각 인덱스에 대한 임베딩 벡터를 가져오고 가중 평균 계산\n",
    "            weights = torch.ones_like(emb_vectors)  # 간단히 모든 값에 대해 동일한 가중치 사용\n",
    "            multihot_embedding = lambda x : torch.mean(embedding(x) * weights, dim=1)\n",
    "\n",
    "        return multihot_embedding\n",
    "        \n",
    "    \n",
    "    def forward(self, x, method_multihot = \"sum\"):\n",
    "        # sparse feature 임베딩\n",
    "        embedded = {}\n",
    "        for k, v in self.datamap.items() :\n",
    "            if k == self.y_col :\n",
    "                continue\n",
    "\n",
    "            embedding = getattr(self, k)\n",
    "\n",
    "            if v == \"multihot\" :\n",
    "                embedding = self._get_multihot_embedding(embedding, method_multihot)\n",
    "\n",
    "            embedding_value = embedding(x[k])\n",
    "            embedded[k] = embedding_value\n",
    "\n",
    "        # 모든 임베딩된 특성을 결합\n",
    "        combined_features = torch.cat([v for k, v in embedded.items()], dim=1)  # dim=1은 각 임베딩을 행 방향으로 결합\n",
    "        \n",
    "        # MLP 레이어 적용\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "aa7ccb19-84b6-4cbc-b6af-394a79d8cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, df, datamap, y_col):\n",
    "        \n",
    "        self.x_data = {}\n",
    "        for col, v in datamap.items() :\n",
    "            if v == \"linear\" :\n",
    "                self.x_data[col] = torch.FloatTensor(df[col].values).unsqueeze(1)\n",
    "            if v == \"multihot\" :\n",
    "                df = df.assign(**{col : lambda x : self._pad_sequence(x[col])})\n",
    "                self.x_data[col] = torch.LongTensor(df[col].to_list())\n",
    "            if v == \"onehot\" :\n",
    "                self.x_data[col] = torch.LongTensor(df[col].values)\n",
    "        \n",
    "        self.y_data = torch.LongTensor(df[y_col].values)\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx):\n",
    "        x = {x : y[idx] for x, y in self.x_data.items()}\n",
    "        y = self.y_data[idx]\n",
    "        return x, y\n",
    "    \n",
    "    def _get_max_multihot_size(self, series):\n",
    "        return series.apply(len).max()\n",
    "\n",
    "    def _get_max_multihot_value(self, series):\n",
    "        return series.apply(max).max()\n",
    "        \n",
    "    def _pad_infinite(self, iterable, padding=None):\n",
    "        from itertools import chain, repeat, islice\n",
    "        return chain(iterable, repeat(padding))\n",
    "    \n",
    "    def _pad(self, iterable, size, padding=None):\n",
    "        return list(islice(pad_infinite(iterable, padding), size))\n",
    "        \n",
    "    def _pad_sequence(self, series) :\n",
    "        l = self._get_max_multihot_size(series)\n",
    "        m = self._get_max_multihot_value(series)\n",
    "        return series.apply(lambda x : self._pad(x, l, m + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "6c7c871a-77ed-4168-b342-76918e1ec456",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamap = {\"a\" : \"linear\", \"b\" : \"multihot\", \"c\" : \"onehot\", \"d\" : \"multihot\", \"y\" : \"onehot\"}\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"a\" : [1.0, 2.0, -1.2], \n",
    "        \"b\" : [[1, 2, 3], [4, 5, 2], [2, 0, 3]], \n",
    "        \"c\" : [1, 2, 3], \n",
    "        \"d\" : [[1, 2, 3], [4, 5, 2], [2, 0, 3]], \n",
    "        \"y\" : [1, 2, 3]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "043a9598-4e42-4080-9e6d-05d72b85d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df, datamap, \"y\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "d75ec1bc-f74c-45f6-b3f9-2caca21da837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "num_sparse_features = 6  # Embedding Idx의 최대값 + 1\n",
    "num_linear_features = 1  # linear feature의 개수 - 개별로 넣으면 1, 묶음이면 len(feature)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 1  # 예측할 출력의 차원 (예: 회귀의 경우 1, 이진 분류의 경우 1)\n",
    "print_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "bd98cb98-be6e-441a-b754-7c69c4a24d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (a): Linear(in_features=1, out_features=10, bias=True)\n",
       "  (b): Embedding(6, 10)\n",
       "  (c): Embedding(6, 10)\n",
       "  (d): Embedding(6, 10)\n",
       "  (y): Embedding(6, 10)\n",
       "  (fc1): Linear(in_features=40, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmbeddingModel(datamap)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "ca8b5ea5-d9fb-4f26-9ef4-1ec54c5f6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0637],\n",
      "        [-0.1611]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1214]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    x, y = samples \n",
    "    # H(x) 계산\n",
    "    output = model(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d750e45c-c50d-4656-8e10-4c7200e99835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Siames Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.model1(input1)\n",
    "        output2 = self.model2(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "81df68c1-fa37-4534-980f-0f5e330ccd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamap1 = {\"a\" : \"linear\", \"b\" : \"multihot\", \"c\" : \"onehot\", \"d\" : \"multihot\", \"y\" : \"onehot\"}\n",
    "\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"a\" : [1.0, 2.0, -1.2], \n",
    "        \"b\" : [[1, 2, 3], [4, 5, 2], [2, 0, 3]], \n",
    "        \"c\" : [1, 2, 3], \n",
    "        \"d\" : [[1, 2, 3], [4, 5, 2], [2, 0, 3]], \n",
    "        \"y\" : [1, 2, 3]}\n",
    ")\n",
    "\n",
    "\n",
    "datamap2 = {\"a\" : \"linear\", \"b\" : \"multihot\", \"c\" : \"onehot\", \"y\" : \"onehot\"}\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"a\" : [1.0, 2.0, -1.2], \n",
    "        \"b\" : [[1, 2, 3], [4, 5, 2], [2, 0, 3]], \n",
    "        \"c\" : [1, 2, 3], \n",
    "        \"y\" : [1, 2, 3]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8f2d5382-4e1d-4af7-898f-cfe27b101ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = CustomDataset(df1, datamap1, \"y\")\n",
    "dataloader1 = DataLoader(dataset1, batch_size=2, shuffle=True)\n",
    "\n",
    "dataset2 = CustomDataset(df2, datamap2, \"y\")\n",
    "dataloader2 = DataLoader(dataset2, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d6a9289a-0415-40bf-9ede-8ea7f81529c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "num_sparse_features = 6  # Embedding Idx의 최대값 + 1\n",
    "num_linear_features = 1  # linear feature의 개수 - 개별로 넣으면 1, 묶음이면 len(feature)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 1  # 예측할 출력의 차원 (예: 회귀의 경우 1, 이진 분류의 경우 1)\n",
    "print_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9ee12fc6-cdda-4b2d-b013-7b2a0b2c787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = EmbeddingModel(datamap1)\n",
    "model2 = EmbeddingModel(datamap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "26b22d01-e438-43fa-8560-721576a2d72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (model1): EmbeddingModel(\n",
       "    (a): Linear(in_features=1, out_features=10, bias=True)\n",
       "    (b): Embedding(6, 10)\n",
       "    (c): Embedding(6, 10)\n",
       "    (d): Embedding(6, 10)\n",
       "    (y): Embedding(6, 10)\n",
       "    (fc1): Linear(in_features=40, out_features=20, bias=True)\n",
       "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       "  (model2): EmbeddingModel(\n",
       "    (a): Linear(in_features=1, out_features=10, bias=True)\n",
       "    (b): Embedding(6, 10)\n",
       "    (c): Embedding(6, 10)\n",
       "    (y): Embedding(6, 10)\n",
       "    (fc1): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siam_model = SiameseNetwork(model1, model2)\n",
    "siam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3e3f565e-2e86-4ee9-987f-1dbfd2479820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6, 10)\n",
      "Embedding(6, 10)\n",
      "Embedding(6, 10)\n",
      "tensor([[ 0.4837],\n",
      "        [-0.1243]], grad_fn=<AddmmBackward0>) tensor([[-0.0366],\n",
      "        [-0.1064]], grad_fn=<AddmmBackward0>)\n",
      "Embedding(6, 10)\n",
      "Embedding(6, 10)\n",
      "Embedding(6, 10)\n",
      "tensor([[-0.0092]], grad_fn=<AddmmBackward0>) tensor([[-0.2142]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(zip(dataloader1, dataloader2)):\n",
    "    _data1 = data[0]\n",
    "    _data2 = data[1]\n",
    "    x1, y1 = _data1\n",
    "    x2, y2 = _data2\n",
    "    \n",
    "    output1, output2 = siam_model(x1, x2)\n",
    "    print(output1, output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a63c89d-f89d-4c58-8aa2-64a23cf54ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_onehot_data(length, *args) :\n",
    "    return [random.randint(*args[0]) for _ in range(length)]\n",
    "\n",
    "def make_linear_data(length, *args) :\n",
    "    return [random.random() * args[0] for _ in range(length)]\n",
    "\n",
    "def make_multihot(length, *args) :\n",
    "    args = args[0]\n",
    "    multihot_length = args[0]\n",
    "    min_range = args[1]\n",
    "    max_range = args[2]\n",
    "    \n",
    "    return [[random.randint(min_range, max_range) for _ in range(multihot_length)] for _ in range(length)]\n",
    "\n",
    "def random_config(feature_type) :\n",
    "\n",
    "    if feature_type == \"onehot\" :\n",
    "        val1 = random.randint(1, 30)\n",
    "        val2 = random.randint(1, 30)\n",
    "        return min(val1, val2), max(val1, val2)\n",
    "        \n",
    "    if feature_type == \"linear\" :\n",
    "        return random.randint(1, 10)\n",
    "    \n",
    "    if feature_type == \"multihot\" :\n",
    "        val1 = random.randint(1, 30)\n",
    "        val2 = random.randint(1, 30)\n",
    "        return random.randint(1, 10), min(val1, val2), max(val1, val2)\n",
    "    \n",
    "    return\n",
    "\n",
    "def make_independent_data(datamap, length) :\n",
    "\n",
    "    data = {}\n",
    "    \n",
    "    for k, v in datamap.items() :\n",
    "        if v == \"onehot\" :\n",
    "            data[k] = make_onehot_data(length, random_config(v))\n",
    "        if v == \"linear\" :\n",
    "            \n",
    "            data[k] = make_linear_data(length, random_config(v))\n",
    "        if v == \"multihot\" :\n",
    "            data[k] = make_multihot(length, random_config(v))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def make_dataset(target_variable, datamap, length) :\n",
    "    \n",
    "    return pd.concat([make_independent_data(datamap, length).assign(y  = i) for i in range(target_variable)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "7206453f-ef4e-47e9-8c7f-6bd035265f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamap1 = {\"a\" : \"linear\", \"b\" : \"multihot\", \"c\" : \"onehot\", \"d\" : \"multihot\", \"y\" : \"onehot\"}\n",
    "datamap2 = {\"a\" : \"linear\", \"b\" : \"multihot\", \"c\" : \"onehot\", \"y\" : \"onehot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "2dfb0404-add5-4e42-a4ba-64b849131798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = make_dataset(3, datamap1, 1000)\n",
    "data2 = make_dataset(3, datamap2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "0ddcbf25-19b4-45ca-8ab3-fe45f9ebd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamesDataset(Dataset): \n",
    "    def __init__(self, df1, datamap1, df2, datamap2, df_y, y_col = \"y\"):\n",
    "        \n",
    "        self.x_data1 = self._get_x_data(df1, datamap1)\n",
    "        self.x_data2 = self._get_x_data(df2, datamap2)\n",
    "        self.y_data = torch.LongTensor(df_y[y_col].values)\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = {x : y[idx] for x, y in self.x_data1.items()}\n",
    "        x2 = {x : y[idx] for x, y in self.x_data2.items()}\n",
    "        y = self.y_data[idx]\n",
    "        return x1, x2, y\n",
    "\n",
    "    def _get_x_data(self, df, datamap) :\n",
    "        \n",
    "        x_data = {}\n",
    "        for col, v in datamap.items() :\n",
    "            if v == \"linear\" :\n",
    "                x_data[col] = torch.FloatTensor(df[col].values).unsqueeze(1)\n",
    "            if v == \"multihot\" :\n",
    "                df = df.assign(**{col : lambda x : self._pad_sequence(x[col])})\n",
    "                x_data[col] = torch.LongTensor(df[col].to_list())\n",
    "            if v == \"onehot\" :\n",
    "                x_data[col] = torch.LongTensor(df[col].values)\n",
    "\n",
    "        return x_data\n",
    "    \n",
    "    def _get_max_multihot_size(self, series):\n",
    "        return series.apply(len).max()\n",
    "\n",
    "    def _get_max_multihot_value(self, series):\n",
    "        return series.apply(max).max()\n",
    "        \n",
    "    def _pad_infinite(self, iterable, padding=None):\n",
    "        from itertools import chain, repeat, islice\n",
    "        return chain(iterable, repeat(padding))\n",
    "    \n",
    "    def _pad(self, iterable, size, padding=None):\n",
    "        return list(islice(pad_infinite(iterable, padding), size))\n",
    "        \n",
    "    def _pad_sequence(self, series) :\n",
    "        l = self._get_max_multihot_size(series)\n",
    "        m = self._get_max_multihot_value(series)\n",
    "        return series.apply(lambda x : self._pad(x, l, m + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "32c66d24-51e4-494e-9296-81802b8b95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SiamesDataset(data1, datamap1, data2, datamap2, data1, \"y\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "17f37194-a625-437c-8af3-90a1d548c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_num_sparse_features(df, datamap, y_col = \"y\") :\n",
    "    datamap = datamap.copy()\n",
    "    datamap.pop(\"y\")\n",
    "    return max(*[df[k].apply(max).max() if v == \"multihot\" else df[k].max() for k, v in datamap.items() if v in [\"multihot\", \"onehot\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "65ba5cc1-e210-4a4f-865b-b7ee7f6f6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "num_sparse_features = max(search_num_sparse_features(data1, datamap1), search_num_sparse_features(data2, datamap2)) + 1  # Embedding Idx의 최대값 + 1\n",
    "num_linear_features = 1  # linear feature의 개수 - 개별로 넣으면 1, 묶음이면 len(feature)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 20\n",
    "output_dim = 1  # 예측할 출력의 차원 (예: 회귀의 경우 1, 이진 분류의 경우 1)\n",
    "print_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "14e5c478-4f68-459b-b874-399f0e02e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = EmbeddingModel(datamap1)\n",
    "model2 = EmbeddingModel(datamap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a6f69247-3482-462e-854f-5c617eee7071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (model1): EmbeddingModel(\n",
       "    (a): Linear(in_features=1, out_features=100, bias=True)\n",
       "    (b): Embedding(30, 100)\n",
       "    (c): Embedding(30, 100)\n",
       "    (d): Embedding(30, 100)\n",
       "    (y): Embedding(30, 100)\n",
       "    (fc1): Linear(in_features=400, out_features=20, bias=True)\n",
       "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       "  (model2): EmbeddingModel(\n",
       "    (a): Linear(in_features=1, out_features=100, bias=True)\n",
       "    (b): Embedding(30, 100)\n",
       "    (c): Embedding(30, 100)\n",
       "    (y): Embedding(30, 100)\n",
       "    (fc1): Linear(in_features=300, out_features=20, bias=True)\n",
       "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siam_model = SiameseNetwork(model1, model2)\n",
    "siam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "47c97fa9-2e27-4b5b-88a7-576658c67b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "eb15eac6-d5a5-44ed-b115-21e27484ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss -2311185152.0\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 5321580032.0\n",
      "\n",
      "Epoch number 20\n",
      " Current loss -10670310400.0\n",
      "\n",
      "Epoch number 30\n",
      " Current loss -62945775616.0\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 1310.719970703125\n",
      "\n",
      "Epoch number 50\n",
      " Current loss -496305274880.0\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 326192201728.0\n",
      "\n",
      "Epoch number 70\n",
      " Current loss -1345123516416.0\n",
      "\n",
      "Epoch number 80\n",
      " Current loss -5114293125120.0\n",
      "\n",
      "Epoch number 90\n",
      " Current loss 823267688448.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = SiameseNetwork(model1, model2)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.0005 )\n",
    "\n",
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x1, x2, y = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = net(x1, x2)\n",
    "        \n",
    "        loss_contrastive = criterion(output1,output2,y)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "    if epoch %10 == 0 :\n",
    "        print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n",
    "        iteration_number +=10\n",
    "        counter.append(iteration_number)\n",
    "        loss_history.append(loss_contrastive.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "d4267d64-6276-48e6-8bd1-5861dd7cb337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 15.6772,   4.7668,   8.2523,   9.4473,  15.6492, -48.7647, -48.7647,\n",
       "        -48.7647], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(siam_model.model1.b(dataset.__getitem__(0)[0][\"b\"]), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f690957-3a57-4581-b75f-4bd39d55148d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
